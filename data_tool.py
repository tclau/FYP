# -*- coding: utf-8 -*-
"""data_tool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R3XsmSn3AVAeb620sP7ehwIDjmQXZpYI
"""

import matplotlib.pyplot as plt # plotting library
import numpy as np # this module is useful to work with numerical arrays
import pandas as pd 
import random 
import torch
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset,DataLoader,random_split
from torch.utils.data.sampler import SubsetRandomSampler
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
import math
import glob
from os.path import basename

# from google.colab import drive
# drive.mount('/content/drive')

# files = glob.glob("/content/drive/MyDrive/AI_FYP/*1h.csv")

def get_data(files,number_of_days,mode="SMA"):
  if mode!="SMA" and mode!="EMA":
    print("The mode can only be SMA or EMA")
    return 

  NUMBER_OF_DAYS = number_of_days
  window_size = 24 * NUMBER_OF_DAYS
  ma_full = {}
  last_index = {}
  set_type = ['train','val','test']
  y_raw = {}
  for t in set_type:
    ma_full[t] = []
    y_raw[t]=[]
    last_index[t]=0
  last_index_for_coin = {}


  for f in files:
    data_df = pd.read_csv(f)

    data_df = data_df[["close"]].pct_change().dropna()
    ma_df = data_df.copy()

    if mode=="SMA":
      for days in range(2,52):
        name = "SMA_"+str(days)
        ma_df[name]=ma_df["close"].rolling(days,min_periods=1).mean()
        # data_df[name] = data_df["close"]
    elif mode=="EMA":
      for days in range(2,52):
        name = "EMA_"+str(days)
        ma_df[name]=ma_df["close"].ewm(span=days, adjust=True).mean()
        # data_df[name] = data_df["close"]

    ma = ma_df.drop(["close"],axis=1)

    data = data_df.values
    ma = ma.values

    l = data.shape[0]
    idx1 = int(l*0.8)
    idx2 = int(l*0.1)
    data_dict, ma_dict = {}, {}

    data_dict['train'] = data[:idx1]
    data_dict['val'] = data[idx1:idx1+idx2]
    data_dict['test'] = data[idx1+idx2:]

    ma_dict['train'] = ma[:idx1]
    ma_dict['val'] = ma[idx1:idx1+idx2]
    ma_dict['test'] = ma[idx1+idx2:]

    for t in set_type:
      slide = np.lib.stride_tricks.sliding_window_view(ma_dict[t],(window_size+1,ma_dict[t].shape[1])).squeeze().astype(np.float32)
      ma_full[t].append(slide)
      
      slide = np.lib.stride_tricks.sliding_window_view(data_dict[t], (window_size+1,data_dict[t].shape[1])).squeeze().astype(np.float32)
      y_raw[t].append(slide[:,-1])

      last_index[t] += slide.shape[0]
    
    last_index_for_coin[basename(f)[:-7]] = last_index.copy()

    del slide
    del ma

  x, y = {}, {}
  for t in set_type:
    ma_full[t]=np.concatenate(ma_full[t])
    y_raw[t]=np.concatenate(y_raw[t])

  for t in set_type:
    x[t],y[t]=ma_full[t][:,:-1],ma_full[t][:,-1]
    y[t] = y[t].mean(axis=1)

  return x,y,y_raw,last_index_for_coin

class ClassDataset(Dataset):
    def __init__(self, input,output,threshold):
          self.input = torch.tensor(input)

          label = []
          p = 0
          n = 0
          z = 0
          for num in output:
            row = [0,0,0]
            # row = [0,0]
            if num>threshold:
              row[2]=1
              p+=1
            elif num<-threshold:
              row[0]=1
              n+=1
            else:
              row[1]=1
              z+=1
              pass
              # row[1]=1
            label.append(row)
          label = np.array(label).astype(np.float32)
          self.label = torch.tensor(label)
          self.output = torch.tensor(output)
          print("Total:",p+n+z)
          print("P:",p)
          print("Z:",z)
          print("N:",n)

    def __len__(self):
        return len(self.input)

    def __getitem__(self, idx):
        x = self.input[idx].T
        y = self.label[idx]

        return x,y

def get_loader(batch_size,inputs,label,threshold=0.0):
  train_set = ClassDataset(inputs['train'],label['train'],0)
  val_set = ClassDataset(inputs['val'],label['val'],0)
  test_set = ClassDataset(inputs['test'],label['test'],0)

  BATCH_SIZE = batch_size
  train_loader = DataLoader(train_set,batch_size=BATCH_SIZE)
  val_loader = DataLoader(val_set,batch_size=BATCH_SIZE)
  test_loader = DataLoader(test_set,batch_size=BATCH_SIZE)

  return train_loader,val_loader,test_loader

def multi_acc(y_pred, y_test):
    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)
    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1) 
    _, lab = torch.max(y_test,dim=1)   
    
    correct_pred = (y_pred_tags == lab).float()
    acc = correct_pred.sum() / len(correct_pred)
    
    acc = torch.round(acc * 100)
    
    return acc,y_pred_tags

def training(epochs,model,criterion,optimizer,device,train_loader,val_loader):
  for epoch in range(epochs):  # loop over the dataset multiple times

      total_loss = 0.0
      for i, data in enumerate(train_loader, 0):
          # get the inputs; data is a list of [inputs, labels]
          inputs, labels = data
          inputs = inputs.to(device)
          labels = labels.to(device)

          # zero the parameter gradients
          optimizer.zero_grad()
          # print(inputs.size())
          # forward + backward + optimize
          outputs = model(inputs)
          loss = criterion(outputs, labels)
          loss.backward()
          optimizer.step()

          # print statistics
          total_loss += loss.item()
      total_loss /= len(train_loader.dataset)
      if epoch % 1 ==0:
        print('[{}/{}] Loss:'.format(epoch+1, epochs), total_loss)
        val_loss = 0.0
        with torch.no_grad():
          model.eval()
          test_acc = 0
          for inputs,labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            v_loss = criterion(outputs,labels)

            acc ,pred_tag= multi_acc(outputs, labels)
            test_acc += acc.item()
            val_loss += v_loss.item()

          test_acc = test_acc/len(val_loader)
          val_loss = val_loss/len(val_loader)
        print('[{}/{}] Val Loss:'.format(epoch+1, epochs), val_loss)
        print('[{}/{}] Val Accuracy:'.format(epoch+1, epochs), test_acc)
      

  print('[{}/{}] Loss:'.format(epoch+1, epochs), total_loss)
  return model

def testing(model,criterion,optimizer,device,test_loader):
    test_loss = 0.0
    with torch.no_grad():
      model.eval()
      test_acc = 0
      for inputs,labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)

        v_loss = criterion(outputs,labels)
                           
        acc ,pred_tag= multi_acc(outputs, labels)
        test_acc += acc.item()
        test_loss += v_loss.item()
      test_acc = test_acc/len(test_loader)
      test_loss = test_loss/len(test_loader)
    print('Test Loss:', test_loss)
    print('Test Accuracy:', test_acc)

def test_profit(files,x,y,y_raw,last_index_for_coin,model,device,batch_size,threshold=0.0):
  all_signal = []
  signal_return = []
  b_h_return = []
  all_acc = []
  s_index = 0
  for f in files:
    coin_name = basename(f)[:-7]
    end_index = last_index_for_coin[coin_name]['test']
    test_x = x['test'][s_index:end_index]
    test_y = y['test'][s_index:end_index]
    ret = y_raw['test'][s_index:end_index]
    s_index = end_index

    test_set = ClassDataset(test_x,test_y,threshold)

    BATCH_SIZE = batch_size
    test_loader = DataLoader(test_set,batch_size=BATCH_SIZE)

    # print(test_x.shape,test_y.shape,ret.shape)
    with torch.no_grad():
      model.eval()
      test_acc = 0
      prediction = []
      for inputs,labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        acc,pred_tag = multi_acc(outputs, labels)
        prediction.append(pred_tag.detach().cpu().numpy())
        test_acc += acc.item()
      test_acc = test_acc/len(test_loader)

    signal=np.concatenate(prediction,axis=0)

    exp_return = []
    for i in range(len(signal)):
      if signal[i]==2:
        exp_return.append(-1*ret[i])
      elif signal[i]==0:
        exp_return.append(1*ret[i])
      else:
        exp_return.append(0)
    exp_return = np.array(exp_return)

    all_signal.append(signal)
    signal_return.append(exp_return)
    b_h_return.append(ret)
    all_acc.append(test_acc)

  return all_signal,signal_return,b_h_return,all_acc